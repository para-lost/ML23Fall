{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cd CLIP \\\n",
    "&& CUDA_VISIBLE_DEVICES=0 python -W ignore validation.py --data_root_path /mnt/target_image/ --start_epoch 10 --end_epoch 40 --epoch_interval 10 --cache_dataset --cache_rate 0.6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CLIP import validation\n",
    "# 直接调用接口：需要把这个接口放到SAM里面作为query\n",
    "prediction = validation(\"/mnt/target_image/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### baseline code\n",
    "ICCV2023work;\n",
    "UniverSeg enables users to tackle a new segmentation task without the need to train or fine-tune a model, removing the requirement for ML experience and computational burden. The key idea is to have a single global model which adapts to a new segmentation task at inference based on an input example set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from universeg import universeg\n",
    "model = universeg(pretrained=True)\n",
    "# here we need to input support image(example images) and labels as example\n",
    "\n",
    "# To perform a prediction (where B=batch, S=support, H=height, W=width)\n",
    "prediction = model(\n",
    "    target_image,        # (B, 1, H, W)\n",
    "    support_images,      # (B, S, 1, H, W)\n",
    "    support_labels,      # (B, S, 1, H, W)\n",
    ") # -> (B, 1, H, W)\n",
    "\n",
    "# save predictions "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
